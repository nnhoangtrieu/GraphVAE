{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn \n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset \n",
    "import torch_geometric\n",
    "import torch_geometric.nn as gnn\n",
    "from torch_geometric.data import Dataset as gDataset\n",
    "from torch_geometric.data import Data as gData \n",
    "from torch_geometric.loader import DataLoader as gDataLoader\n",
    "from torch_geometric.datasets import QM9, ZINC\n",
    "from tqdm import tqdm \n",
    "from tqdm.auto import tqdm as loading\n",
    "import rdkit\n",
    "from rdkit.Chem import MolFromSmiles as get_mol\n",
    "from rdkit.Chem.rdmolops import GetAdjacencyMatrix as get_mat\n",
    "import numpy as np \n",
    "import os \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_smi(path) :\n",
    "    with open(path, 'r') as file :\n",
    "        contents = file.readlines() \n",
    "    smi_list = [content[:-1] for content in contents]\n",
    "    return smi_list\n",
    "\n",
    "def get_coor(path) :\n",
    "    coor_list = []\n",
    "    supplier = rdkit.Chem.SDMolSupplier(path)\n",
    "    for mol in supplier:\n",
    "        coor = []\n",
    "        if mol is not None:\n",
    "            conformer = mol.GetConformer()\n",
    "            for atom in mol.GetAtoms():\n",
    "                atom_idx = atom.GetIdx()\n",
    "                x, y, z = conformer.GetAtomPosition(atom_idx)\n",
    "                coor_atom = list((x,y,z))\n",
    "                coor.append(coor_atom)\n",
    "        coor_list.append(coor)\n",
    "\n",
    "    # Replace invalid idx\n",
    "    for i, coor in enumerate(coor_list):\n",
    "        \n",
    "        if len(coor) == 0 :\n",
    "            if i == 0 :\n",
    "                coor_list = coor_list[1:]\n",
    "            coor_list[i] = coor_list[i-1]\n",
    "    return coor_list\n",
    "\n",
    "def get_edge_index(mol) :\n",
    "    edge_indices, begin, end = [], [], []\n",
    "\n",
    "    for bond in mol.GetBonds() :\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx() \n",
    "        begin.append(i), end.append(j)\n",
    "\n",
    "    for bond in mol.GetBonds() :\n",
    "        i = bond.GetBeginAtomIdx()\n",
    "        j = bond.GetEndAtomIdx()\n",
    "        begin.append(j), end.append(i)\n",
    "    edge_indices.append(begin), edge_indices.append(end)\n",
    "\n",
    "    return torch.tensor(edge_indices, dtype=torch.long)\n",
    "\n",
    "def get_edge_features( mol) :\n",
    "    all_edge_feats = [] \n",
    "\n",
    "    for bond in mol.GetBonds() :\n",
    "        edge_feats = []\n",
    "\n",
    "        edge_feats.append(bond.GetBondType())\n",
    "        all_edge_feats.append(edge_feats)\n",
    "    \n",
    "    for bond in mol.GetBonds() :\n",
    "        edge_feats = []\n",
    "\n",
    "        edge_feats.append(bond.GetBondType())\n",
    "        all_edge_feats.append(edge_feats)\n",
    "\n",
    "    all_edge_feats = np.asarray(all_edge_feats)\n",
    "    return torch.tensor(all_edge_feats, dtype=torch.float)\n",
    "\n",
    "def get_node_features(mol, atom_dic) :\n",
    "    all_node_feats = [] \n",
    "\n",
    "    for atom in mol.GetAtoms() :\n",
    "        node_feats = []\n",
    "        # node_feats.append(atom.GetAtomicNum())\n",
    "        # node_feats.append(atom.GetDegree())\n",
    "        # node_feats.append(atom.GetFormalCharge())\n",
    "        # node_feats.append(atom.GetHybridization())\n",
    "        # node_feats.append(atom.GetIsAromatic())\n",
    "        # node_feats.append(atom.GetTotalNumHs())\n",
    "        # node_feats.append(atom.GetNumRadicalElectrons())\n",
    "        # node_feats.append(atom.IsInRing())\n",
    "        # node_feats.append(atom.GetChiralTag())\n",
    "\n",
    "        node_feats.append(atom_dic[atom.GetSymbol()])\n",
    "\n",
    "        all_node_feats.append(node_feats)\n",
    "\n",
    "    # all_node_feats = np.asarray(all_node_feats)\n",
    "    return torch.tensor(all_node_feats, dtype=torch.long).squeeze(-1)\n",
    "\n",
    "def count_atoms(smi):\n",
    "    mol = rdkit.Chem.MolFromSmiles(smi)\n",
    "    if mol is not None:\n",
    "        num_atoms = mol.GetNumAtoms()\n",
    "        return num_atoms\n",
    "    else:\n",
    "        print(\"Error: Unable to parse SMILES string.\")\n",
    "        return None\n",
    "\n",
    "def get_atom(smi) :\n",
    "    mol = get_mol(smi) \n",
    "    atom_list = [x.GetSymbol() for x in mol.GetAtoms()]\n",
    "    return atom_list\n",
    "\n",
    "def get_bond(smi) :\n",
    "    mol = get_mol(smi) \n",
    "    bond_list = [x.GetBondType() for x in mol.GetBonds()]\n",
    "    return bond_list \n",
    "\n",
    "def get_atom_mat(smi, max_atom) :\n",
    "    mat = get_mat(get_mol(smi))\n",
    "    pad = np.zeros((max_atom, max_atom))\n",
    "    for i, (m, w) in enumerate(zip(mat, pad)) :\n",
    "        ones = np.where(m == 1)[0]\n",
    "\n",
    "        for idx in ones :\n",
    "            pad[i][idx] = 1\n",
    "            # wmat[i][idx] = atom_dic[atom_list[idx]]\n",
    "    return torch.tensor(pad, dtype = torch.float)\n",
    "\n",
    "def get_bond_mat(smi, max_atom) :\n",
    "    mol = get_mol(smi) \n",
    "    mat = get_mat(mol)\n",
    "\n",
    "    wmat = np.zeros((max_atom, max_atom))\n",
    "\n",
    "    for bond in mol.GetBonds() :\n",
    "        b, e = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "\n",
    "        wmat[b][e] = bond_dic[str(bond.GetBondType())]\n",
    "        wmat[e][b] = bond_dic[str(bond.GetBondType())]\n",
    "\n",
    "    return torch.tensor(wmat, dtype = torch.long)\n",
    "\n",
    "def get_dic(smi_list) :\n",
    "    a_dic, b_dic, i, j  = {}, {}, 1, 1 \n",
    "\n",
    "    for smi in smi_list :\n",
    "        mol = get_mol(smi) \n",
    "        for atom in mol.GetAtoms() :\n",
    "            symbol = atom.GetSymbol() \n",
    "            if symbol not in a_dic : \n",
    "                a_dic[symbol] = i; i += 1\n",
    "\n",
    "        for bond in mol.GetBonds() :\n",
    "            bond_type = str(bond.GetBondType())\n",
    "            if bond_type not in b_dic : \n",
    "                b_dic[bond_type] = j; j += 1 \n",
    "    return a_dic, b_dic \n",
    "    \n",
    "\n",
    "def pad_bond(tensor, max_atom) :\n",
    "    row, col = tensor.size() \n",
    "    pad = torch.cat((tensor, torch.zeros(row, max_atom - col)), dim = 1)\n",
    "    pad = torch.cat((pad, torch.zeros(max_atom - row, pad.size(1))), dim = 0)\n",
    "    return pad \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "smi_list = get_smi('ADAGRASIB_SMILES.txt')\n",
    "\n",
    "# smi_list = [smi for smi in smi_list if count_atoms(smi) < 9]\n",
    "atom_dic, bond_dic = get_dic(smi_list)\n",
    "\n",
    "atom_dic['None'] = 0 \n",
    "bond_dic['None'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1,\n",
       " 'N': 2,\n",
       " 'B': 3,\n",
       " 'O': 4,\n",
       " 'F': 5,\n",
       " 'S': 6,\n",
       " 'Cl': 7,\n",
       " 'K': 8,\n",
       " 'Br': 9,\n",
       " 'P': 10,\n",
       " 'Na': 11,\n",
       " 'Ba': 12,\n",
       " 'None': 0}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "atom_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyData(gData) :\n",
    "    def __cat_dim__(self, key, value, *args, **kwargs):\n",
    "        if key == 'atom_mat' or key == 'bond_mat':\n",
    "            return None\n",
    "        return super().__cat_dim__(key, value, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(gDataset) : \n",
    "    def __init__(self, root, filename, transform = None, pre_transform = None) :\n",
    "        self.filename = filename \n",
    "        super(MyDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self) :\n",
    "        return self.filename \n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self) :\n",
    "        self.smi_list = get_smi(self.raw_paths[0])\n",
    "        # self.smi_list = [smi for smi in self.smi_list if count_atoms(smi) < 9]\n",
    "        return [f'data_{i}.pt' for i in range(len(self.smi_list))]\n",
    "    \n",
    "    def download(self) : pass \n",
    "\n",
    "    def process(self) :\n",
    "        smi_list = get_smi(self.raw_paths[0])\n",
    "        # smi_list = [smi for smi in smi_list if count_atoms(smi) < 9]\n",
    "        atom_dic, _ = get_dic(smi_list) \n",
    "\n",
    "        atom_dic['None'] = 0\n",
    "        \n",
    "        max_atom = count_atoms(max(smi_list, key = lambda x : count_atoms(x)))\n",
    "        \n",
    "        # atom_mat_list = [get_mat(get_mol(smi)) for smi in smi_list]\n",
    "        atom_mat_list = [get_atom_mat(smi, max_atom) for smi in smi_list]\n",
    "    \n",
    "        for i, smi in enumerate(tqdm(smi_list, total=len(smi_list))) :\n",
    "            mol = get_mol(smi) \n",
    "\n",
    "            node_feat = get_node_features(mol, atom_dic)\n",
    "            edge_idx = get_edge_index(mol)\n",
    "            edge_attr = get_edge_features(mol)\n",
    "\n",
    "            data = MyData(x = node_feat,\n",
    "                        edge_index = edge_idx,\n",
    "                        edge_attr=edge_attr,\n",
    "                        atom_mat = atom_mat_list[i],\n",
    "                        )\n",
    "\n",
    "            torch.save(data, os.path.join(self.processed_dir, f'data_{i}.pt'))\n",
    "\n",
    "\n",
    "    def len(self) :\n",
    "        return len(self.smi_list)\n",
    "    \n",
    "    def get(self, idx) :\n",
    "        data = torch.load(os.path.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(root='data', filename=['ADAGRASIB_SMILES.txt'])\n",
    "train_loader = gDataLoader(dataset, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAE(nn.Module) :\n",
    "#     def __init__(self, dim_latent) :\n",
    "#         super(VAE, self).__init__()\n",
    "#         self.N = torch.distributions.Normal(0, 1)\n",
    "#         self.N.loc = self.N.loc.cuda() # hack to get sampling on the GPU\n",
    "#         self.N.scale = self.N.scale.cuda()\n",
    "#         self.kl = 0\n",
    "\n",
    "#         self.gc1 = gnn.GCNConv(9, 128)\n",
    "#         self.gc1_bn = nn.BatchNorm1d(128)\n",
    "#         self.gc2 = gnn.GCNConv(128, 128) \n",
    "#         self.gc2_bn = nn.BatchNorm1d(128)\n",
    "        \n",
    "#         self.ff1 = nn.Linear(128, 128) \n",
    "#         self.ff1_bn = nn.BatchNorm1d(128)\n",
    "\n",
    "#         self.mu = nn.Linear(128, dim_latent)\n",
    "#         self.sigma = nn.Linear(128, dim_latent) \n",
    "    \n",
    "#         self.a_emb = nn.Embedding(len(atom_dic), 128) \n",
    "#         self.b_emb = nn.Embedding(len(bond_dic), 128)\n",
    "\n",
    "#         self.a_seq = nn.Sequential(\n",
    "#             nn.Linear(dim_latent, 256),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(256, 512),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(512, len(atom_dic))\n",
    "#         )\n",
    "\n",
    "#         self.b_seq = nn.Sequential(\n",
    "#             nn.Linear(dim_latent, 256),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(256, 512),\n",
    "#             nn.LeakyReLU(),\n",
    "#             nn.Linear(512, len(bond_dic))\n",
    "#         )\n",
    "#     def encode(self, input) :\n",
    "#         x, edge_index, batch, atom_mat, bond_mat = input.x, input.edge_index, input.batch, input.atom_mat, input.bond_mat\n",
    "\n",
    "#         atom_mat = self.a_emb(atom_mat) \n",
    "#         bond_mat = self.b_emb(bond_mat) \n",
    "\n",
    "#         x = self.gc1(x, edge_index)\n",
    "#         x = F.relu(self.gc1_bn(x))\n",
    "#         x = self.gc2(x, edge_index)\n",
    "#         x = F.relu(self.gc2_bn(x))\n",
    "#         x = gnn.global_add_pool(x, batch) \n",
    "#         x = self.ff1(x) \n",
    "#         x = F.relu(self.ff1_bn(x))\n",
    "        \n",
    "\n",
    "        \n",
    "#         x = x.unsqueeze(1).unsqueeze(1)\n",
    "\n",
    "#         x = x + atom_mat + bond_mat\n",
    "\n",
    "#         mu, sigma = self.mu(x), self.sigma(x) \n",
    "\n",
    "#         z = mu + sigma * self.N.sample(mu.shape)\n",
    "    \n",
    "#         # print((sigma ** 2 + mu ** 2 - torch.log(sigma) - 1/2))\n",
    "#         # self.kl = (sigma ** 2 + mu ** 2 - torch.log(sigma) - 1/2).sum()\n",
    "#         self.kl = 0.5 * torch.sum(torch.exp(sigma) + mu**2 - 1.0 - sigma) / 128\n",
    "#         return z \n",
    "\n",
    "#     def decode(self, z) :\n",
    "#         atom_mat, bond_mat = self.a_seq(z), self.b_seq(z) \n",
    "#         return atom_mat, bond_mat\n",
    "    \n",
    "#     def forward(self, x) :\n",
    "#         z = self.encode(x) \n",
    "#         atom_mat, bond_mat = self.decode(z)\n",
    "#         atom_mat = F.log_softmax(atom_mat, dim = -1)\n",
    "#         bond_mat = F.log_softmax(bond_mat, dim = -1)\n",
    "#         return atom_mat, bond_mat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GVAE(nn.Module) :\n",
    "    def __init__(self, dim_model, dim_latent) :\n",
    "        super(GVAE, self).__init__()\n",
    "\n",
    "        self.kl = 0 \n",
    "\n",
    "        self.node_embedding = nn.Embedding(len(atom_dic), dim_model)\n",
    "\n",
    "        self.gc1 = gnn.GCNConv(dim_model, dim_model)\n",
    "        self.gc2 = gnn.GCNConv(dim_model, dim_model * 2)\n",
    "        self.gc1_bn = nn.BatchNorm1d(dim_model)\n",
    "        self.gc2_bn = nn.BatchNorm1d(dim_model * 2)\n",
    "\n",
    "\n",
    "        self.mu = gnn.GCNConv(dim_model * 2, dim_latent)\n",
    "        self.sigma = gnn.GCNConv(dim_model * 2, dim_latent)\n",
    "\n",
    "\n",
    "        self.node_dec = nn.Sequential(\n",
    "            nn.Linear(dim_latent,dim_model),\n",
    "            nn.BatchNorm1d(dim_model),\n",
    "            nn.Linear(dim_model, len(atom_dic)),\n",
    "            nn.BatchNorm1d(len(atom_dic))\n",
    "        )\n",
    "\n",
    "        self.mat_dec = nn.Sequential(\n",
    "            nn.Linear(dim_latent, dim_model),\n",
    "            nn.BatchNorm1d(dim_model),\n",
    "            nn.Linear(dim_model, 22 * 22),\n",
    "            nn.BatchNorm1d(22 * 22)\n",
    "        )\n",
    "\n",
    "    def encode (self, input) :\n",
    "        x, edge_index, batch = input.x, input.edge_index, input.batch \n",
    "        \n",
    "        x = self.node_embedding(x) \n",
    "\n",
    "        x = self.gc1(x, edge_index)\n",
    "        x = F.relu(self.gc1_bn(x))\n",
    "        x = self.gc2(x, edge_index)\n",
    "        x = F.relu(self.gc2_bn(x))\n",
    "\n",
    "        mu, sigma = self.mu(x, edge_index), self.sigma(x, edge_index) \n",
    "\n",
    "        eps = torch.randn_like(sigma).to(mu.device)\n",
    "\n",
    "        z = mu + eps * torch.exp(sigma)\n",
    "\n",
    "        self.kl = 0.5 * torch.sum(torch.exp(sigma) + mu**2 - 1.0 - sigma) \n",
    "\n",
    "        return z, batch\n",
    "\n",
    "    def decode(self, z, batch) :   \n",
    "        pool = gnn.global_add_pool(z, batch)\n",
    "\n",
    "        node = self.node_dec(z) \n",
    "        mat = self.mat_dec(pool)\n",
    "\n",
    "        return node, mat\n",
    "    def forward(self, input) :\n",
    "        z, batch = self.encode(input) \n",
    "\n",
    "        node, mat = self.decode(z, batch) \n",
    "        \n",
    "        node = F.log_softmax(node, dim = -1)\n",
    "        mat = F.sigmoid(mat)\n",
    "\n",
    "        return node, mat \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIM_MODEL = 256 \n",
    "DIM_LATENT = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GVAE(DIM_MODEL, DIM_LATENT).to(device)\n",
    "node_loss_fn = nn.NLLLoss( reduction='mean')\n",
    "mat_loss_fn = nn.BCELoss()\n",
    "\n",
    "optim = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|████████████████████████████████████▊                                                                                                                                                   | 1/5 [00:02<00:09,  2.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 7.5401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|█████████████████████████████████████████████████████████████████████████▌                                                                                                              | 2/5 [00:04<00:07,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 3.7981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                                                                         | 3/5 [00:07<00:04,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 2.1915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                                    | 4/5 [00:09<00:02,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 1.4652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 5/5 [00:11<00:00,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 1.0923\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, 6)) :\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for data in train_loader :\n",
    "        optim.zero_grad()\n",
    "        node, mat = model(data) \n",
    "\n",
    "        node_loss = node_loss_fn(node, data.x) \n",
    "        mat_loss = mat_loss_fn(mat, data.atom_mat.view(-1, 22 * 22))\n",
    "\n",
    "        loss = node_loss + mat_loss + model.kl \n",
    "        \n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_loss = epoch_loss / len(train_loader)\n",
    "    print(f'Epoch {epoch}: {epoch_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0., 1., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n",
      "tensor([1, 4, 1, 4, 1, 2, 4, 2, 1, 2], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "NUM_NODE = 10\n",
    "\n",
    "z = torch.randn(NUM_NODE, DIM_LATENT).to(device)\n",
    "batch = torch.zeros(NUM_NODE, dtype=torch.long).to(device)\n",
    "\n",
    "model.eval()\n",
    "node, mat = model.decode(z, batch)\n",
    "\n",
    "mat = mat.view(22,22)\n",
    "binary = (mat > 0.5).float()\n",
    "extract = binary[:NUM_NODE,:NUM_NODE]\n",
    "\n",
    "print(extract)\n",
    "\n",
    "_, idx = torch.topk(node, 1, dim = -1)\n",
    "idx = idx.squeeze(-1).squeeze(0)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
